{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Etsy Product Scraping With Scrapy.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPq5GTleCX/lhVat+rsV7K6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ctclumak/posts/blob/master/Etsy_Product_Scraping_With_Scrapy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A9tTcLCm0OzA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61d072ce-912c-49d8-f1f7-f42f77a6181a"
      },
      "source": [
        "pip install scrapy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scrapy in /usr/local/lib/python3.7/dist-packages (2.4.1)\n",
            "Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.1.0)\n",
            "Requirement already satisfied: itemadapter>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.2.0)\n",
            "Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (20.0.1)\n",
            "Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.6.0)\n",
            "Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (21.2.0)\n",
            "Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.22.0)\n",
            "Requirement already satisfied: lxml>=3.5.0; platform_python_implementation == \"CPython\" in /usr/local/lib/python3.7/dist-packages (from scrapy) (4.2.6)\n",
            "Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.5.0)\n",
            "Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.7/dist-packages (from scrapy) (0.1.16)\n",
            "Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (3.4.6)\n",
            "Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.7/dist-packages (from scrapy) (5.2.0)\n",
            "Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.7/dist-packages (from scrapy) (18.1.0)\n",
            "Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.7/dist-packages (from scrapy) (2.0.5)\n",
            "Requirement already satisfied: itemloaders>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from scrapy) (1.0.4)\n",
            "Requirement already satisfied: six>=1.5.2 in /usr/local/lib/python3.7/dist-packages (from pyOpenSSL>=16.2.0->scrapy) (1.15.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (20.3.0)\n",
            "Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (21.0.0)\n",
            "Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n",
            "Requirement already satisfied: Automat>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n",
            "Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.7/dist-packages (from Twisted>=17.9.0->scrapy) (21.3.0)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.7/dist-packages (from cryptography>=2.0->scrapy) (1.14.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from zope.interface>=4.1.3->scrapy) (54.0.0)\n",
            "Requirement already satisfied: pyasn1 in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n",
            "Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.7/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n",
            "Requirement already satisfied: jmespath>=0.9.5 in /usr/local/lib/python3.7/dist-packages (from itemloaders>=1.0.1->scrapy) (0.10.0)\n",
            "Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.7/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.10)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.12->cryptography>=2.0->scrapy) (2.20)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGD2XyTGzJ9B"
      },
      "source": [
        "import scrapy\n",
        "import pandas as pd\n",
        "from scrapy.crawler import CrawlerProcess\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BoDMChJcadTq"
      },
      "source": [
        "#--------Define the variables\n",
        "\n",
        "#This holds the listing url\n",
        "list_id_records = []\n",
        "url_lists = []\n",
        "keywords_records=[]\n",
        "\n",
        "query = \"wedding+accessories\"\n",
        "class EtsySpider(scrapy.Spider):\n",
        "  name = \"Etsy_crawler\"\n",
        "  allowed_domains = ['etsy.com']\n",
        "  base_url = \"https://www.etsy.com/search/?q=%s&ref=pagination&page=%d\"\n",
        "#--------creating list of urls to be scraped\n",
        "  for i in range(1,5):\n",
        "    url = base_url % (query,i)\n",
        "    url_lists.append(url)\n",
        "#------Define a Request\n",
        "  def start_requests(self):\n",
        "    for url in url_lists:\n",
        "      return scrapy.Request(url=url,callback=self.parse)\n",
        "\n",
        "#--------Defining a Scrapy parse\n",
        "  #parse() is the Scrapyâ€™s default callback method which is called for requests without an explicitly assigned callback\n",
        "  def parse(self,response):\n",
        "\n",
        "      # #Get the listing id\n",
        "      list_id = response.css(\"a::attr(data-listing-id)\").get_all()\n",
        "\n",
        "      if list_id != None:\n",
        "            url_product = \"http://www.etsy.com/listing/\" + str(list_id) +\"/\"\n",
        "            list_id_records.append(url_product)\n",
        "\n",
        "      for list_id in list_id_records:\n",
        "        yield {\n",
        "        \"list_id_no\": response.css(\"h1::attr(data-listing-id)\").get(),\n",
        "        \"title\": response.css(\"div.wt-mb-xs-2::text\").get(),\n",
        "        \"seller\": response.css(\"a.wt-text-link::text\").get(),\n",
        "        \"price\": response.css(\"p.wt-text-title-03 wt-mr-xs-2::text\").get(),\n",
        "        \"total_sales\": response.css(\"span.wt-text-caption::text\").get(),\n",
        "        \"description\": response.css(\"p.wt-text-body-01 wt-break-word::text\").get(),\n",
        "        \"review\": response.css(\"h3.wt-text-body-03::text\").get(),\n",
        "        \"website\": response.css(\"a.wt-text-link::attr(href)\").get()   \n",
        "         }\n",
        "   \n",
        "        keywords_records.append((list_id_no, title, seller, price, total_sales,description, review, website))\n",
        "        df = pd.DataFrame(product, columns =[\"List_ID\", \"Title\", \"Seller\",\"Current_Price\", \"Product_Sales\", \"Product_Detail\", \"Total_Review\", \"Website\"])\n",
        "        df_main = pd.DataFrame()\n",
        "        df_main = pd.concat([df_main,df])\n",
        "        time.sleep(random.randint(1,3))\n",
        "        df_main.to_csv('etsy_products.csv', index=False, encoding='utf-8')\n"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKPKPPnMMnGE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLZ0wZHTY3WN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bb7bd451-bbf7-47a3-c617-2f860d8f2213"
      },
      "source": [
        "process = CrawlerProcess()\n",
        "process.crawl(EtsySpider)\n",
        "process.start() # the script will block here until all crawling jobs are finished"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-03-09 13:39:31 [scrapy.utils.log] INFO: Scrapy 2.4.1 started (bot: scrapybot)\n",
            "2021-03-09 13:39:31 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 21.2.0, Python 3.7.10 (default, Feb 20 2021, 21:17:23) - [GCC 7.5.0], pyOpenSSL 20.0.1 (OpenSSL 1.1.1j  16 Feb 2021), cryptography 3.4.6, Platform Linux-4.19.112+-x86_64-with-Ubuntu-18.04-bionic\n",
            "2021-03-09 13:39:31 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2021-03-09 13:39:31 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2021-03-09 13:39:31 [scrapy.extensions.telnet] INFO: Telnet Password: abd7a0ef6f288ef5\n",
            "2021-03-09 13:39:31 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2021-03-09 13:39:31 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2021-03-09 13:39:31 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2021-03-09 13:39:31 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wpuSARdpzhH1"
      },
      "source": [
        "# #This holds the listing url\n",
        "# list_id_records = []\n",
        "# keywords_records = []\n",
        "\n",
        "# class EtsySpider(scrapy.Spider):\n",
        "#   name = \"Etsy_crawler\"\n",
        "#   allowed_domains = ['etsy.com']\n",
        "#   base_url = \"https://www.etsy.com/search/?q=%s&ref=pagination&page=%d\"\n",
        "#   #parse() is the Scrapyâ€™s default callback method which is called for requests without an explicitly assigned callback\n",
        "#   def parse(self,response):\n",
        "#       url = response.url    \n",
        "#       #This is the listing id list\n",
        "#       listing_id = response.find_all(\"a\")\n",
        "#       time.sleep(random.randint(1,3))\n",
        "#   #this gather listing url by listing id and adding to website address\n",
        "#       for listing in listing_id:\n",
        "#         list_id = (listing.get(\"data-listing-id\"))\n",
        "#         if list_id != None:\n",
        "#             url_product = \"http://www.etsy.com/listing/\" + str(list_id) +\"/\"\n",
        "#             list_id_records.append(url_product)\n",
        "\n",
        "#       for list_id in list_id_records:\n",
        "#         etsy_page_product = requests.get(list_id)\n",
        "#         soup_product = BeautifulSoup(etsy_page_product.content, \"html.parser\")\n",
        "#         list_id_no = soup_product.select(\"a.listing-link\")[0].get(\"data-listing-id\")\n",
        "#         title = soup_product.find(\"div\", {\"data-component\":\"listing-page-title-component\"}).text.strip()\n",
        "#         #seller = soup_product.select(\".wt-mb-xs-1 p a.wt-text-link-no-underline\")[0].find(\"span\").text.strip().replace(\"\\n\",\" \")\n",
        "#         price = soup_product.find(\"p\", {\"class\":\"wt-text-title-03 wt-mr-xs-2\"}).text.strip().replace(\"\\n\",\" \").replace(\"Price: \",\"\").replace(\"+\",\"\")\n",
        "#         try:\n",
        "#           total_sales = soup_product.select(\".wt-display-inline-flex-xs a.wt-text-link-no-underline\")[0].find(\"span\").text.strip().replace(\"\\n\",\" \").replace(\"sales\", \"\").strip()\n",
        "#         except:\n",
        "#           total_sales =0\n",
        "#         description = soup_product.find(\"p\", {\"class\":\"wt-text-body-01 wt-break-word\"}).text.strip().replace(\"\\n\",\" \")\n",
        "#         product_url = \"http://www.etsy.com/listing/\" + str(list_id_no) +\"/\"\n",
        "#         try: \n",
        "#           review = soup_product.find(\"h3\", {\"class\":\"wt-text-body-03\"}).text.strip().replace(\"\\n\",\",\").replace(\",\",\"\").replace(\"shop\",\"\")\n",
        "#         except:\n",
        "#           review =0\n",
        "#         #website = soup_product.select(\".wt-mb-xs-1 p a.wt-text-link-no-underline\")[0].get(\"href\")\n",
        "#         keywords_records.append((list_id_no, title, price, total_sales,description, product_url, review))\n",
        "#         df = pd.DataFrame(keywords_records, columns =[\"List_ID\", \"Title\",\"Current_Price\", \"Product_Sales\", \"Product_Detail\", \"Product_list\", \"Total_Review\"])\n",
        "#         df_main = pd.DataFrame()\n",
        "#         df_main = pd.concat([df_main,df])\n",
        "#         time.sleep(random.randint(1,3))\n",
        "#         df_main.to_csv('etsy_products.csv', index=False, encoding='utf-8')\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRHkExihJRC2"
      },
      "source": [
        "      # for list_id in list_id_records:\n",
        "      #   product = []\n",
        "      #   product[\"list_id_no\"]=response.css(\"h1::attr(data-listing-id)\").get(),\n",
        "      #   product[\"title\"]=response.css(\"div.wt-mb-xs-2::text\").get(),\n",
        "      #   product[\"seller\"]=response.css(\"a.wt-text-link::text\").get(),\n",
        "      #   product[\"price\"]=response.css(\"p.wt-text-title-03 wt-mr-xs-2::text\").get(),\n",
        "      #   product[\"total_sales\"]=response.css(\"span.wt-text-caption::text\").get(),\n",
        "      #   product[\"description\"]=response.css(\"p.wt-text-body-01 wt-break-word::text\").get(),\n",
        "      #   product[\"review\"]=response.css(\"h3.wt-text-body-03::text\").get(),\n",
        "      #   product[\"website\"]=response.css(\"a.wt-text-link::attr(href)\").get()   \n",
        "      #   return product"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}